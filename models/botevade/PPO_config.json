{"algorithm": "PPO",
 "environment_count": 5,
 "use_predator": true,
 "max_step": 300,
 "step_wait": 10,
 "world_name": "21_05",
 "training_steps": 200000,
 "network_architecture": [256, 256],
 "learning_rate": 3e-4,
 "log_interval": 1,
 "batch_size": 256,
 "n_steps": 128,
 "reward_structure": {"puffed":  -1, "finished": 1}}